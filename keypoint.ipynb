{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3/19 과제 코드.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "296975022c364d0386eac1a988ce1bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41a515e97546457b9f9ad0b2910fec41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33c03d5d1ced418497d542b4abd6c640",
              "IPY_MODEL_312bee186e6046c0ac827ebcea9bc9b7"
            ]
          }
        },
        "41a515e97546457b9f9ad0b2910fec41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33c03d5d1ced418497d542b4abd6c640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd262bbd15ea45608193c4761abe163a",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1049,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e46db937f6104628b93f71d087abbe14"
          }
        },
        "312bee186e6046c0ac827ebcea9bc9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_680a6fd385bc4704b6b6566cbe0f741e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/1049 [00:03&lt;30:43,  1.76s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1cd84fc40526473bb2bd19807e56f316"
          }
        },
        "cd262bbd15ea45608193c4761abe163a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e46db937f6104628b93f71d087abbe14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "680a6fd385bc4704b6b6566cbe0f741e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1cd84fc40526473bb2bd19807e56f316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNPYQE2r7oDJ"
      },
      "source": [
        "#1. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvKRcPXmFbjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67d5cff-7a4f-4079-96d3-b0d0f4b64309"
      },
      "source": [
        "! pip install albumentations==0.4.6\n",
        "import torch,gc\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torchvision.models.detection import KeypointRCNN\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Tuple, List, Sequence, Callable, Dict"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55SCipZKD4Av"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPzwS5ky8Qbw"
      },
      "source": [
        "# 2. 코랩 연결 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsrq745eAy5k",
        "outputId": "f0973e5f-e543-4a1a-fd8e-6f110774f428"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "root_dir = '/content/drive/MyDrive/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPPO8rFTANWs"
      },
      "source": [
        "feature_extracting=True\n",
        "num_classes = 48\n",
        "learning_rate = 1e-4\n",
        "batch_size = 4\n",
        "num_epochs = 1000\n",
        "test_dir = 'data/test_imgs'\n",
        "train_dir = \"data/train_imgs\"\n",
        "train_df_csv = \"data/train_df.csv\"\n",
        "test_imgs = os.listdir(os.path.join(root_dir,test_dir))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzW2W51j8ce2"
      },
      "source": [
        "# 3. 함수 정의 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w_UC1rXSrR7"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpgIox1NAtz4"
      },
      "source": [
        "class KeypointDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_path, phase, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.df = pd.read_csv(label_path)\n",
        "        self.transforms = transforms\n",
        "        self.phase= phase\n",
        "    def __len__(self) -> int:\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index) -> Tuple[Tensor, Dict]:\n",
        "        image_id = self.df.iloc[index, 0]\n",
        "        labels = np.array([1])\n",
        "        keypoints = self.df.iloc[index, 1:].values.reshape(-1, 2).astype(np.int64)\n",
        "\n",
        "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
        "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
        "        boxes = np.array([[x1, y1, x2, y2]], dtype=np.int64)\n",
        "\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_id), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        targets ={\n",
        "            'image': image,\n",
        "            'bboxes': boxes,\n",
        "            'labels': labels,\n",
        "            'keypoints': keypoints\n",
        "        }\n",
        "\n",
        "        if self.transforms is not None:\n",
        "          targets = self.transforms[self.phase](**targets)\n",
        "        \n",
        "        image = targets['image']\n",
        "        image = image / 255.0\n",
        "\n",
        "        targets = {\n",
        "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
        "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
        "            'keypoints': torch.as_tensor(\n",
        "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
        "            )\n",
        "        }\n",
        "\n",
        "        return image, targets"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k10v5KC_5IJ"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
        "    def __init__(self, data_dir, imgs, phase, transforms=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.imgs = imgs\n",
        "        self.phase = phase\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.imgs[idx]\n",
        "        # Read an image with OpenCV\n",
        "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
        "\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms[self.phase](image=img)\n",
        "            img = augmented['image']\n",
        "\n",
        "        img = img / 255.0\n",
        "        return filename, img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "  \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RENpSKfaBfRp"
      },
      "source": [
        "def get_model() -> nn.Module:\n",
        "    backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0', '1', '2', '3'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    keypoint_roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0', '1', '2', '3'],\n",
        "        output_size=14,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    model = KeypointRCNN(\n",
        "        backbone, \n",
        "        num_classes=2,\n",
        "        num_keypoints=24,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        keypoint_roi_pool=keypoint_roi_pooler\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipxM55g2Duq0"
      },
      "source": [
        "def set_parameter_requires_grad(model,feature_extracting):\n",
        "  if feature_extracting:\n",
        "    for param in model.backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "      # False로 바뀐 부분을 학습 안하겠다.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB8TRdM0P785"
      },
      "source": [
        "# augmentation \n",
        "A_transforms = {\n",
        "    'train':\n",
        "        A.Compose([\n",
        "            A.Resize(224, 224, always_apply=True),\n",
        "            A.Rotate(limit=40,p=0.9),\n",
        "            A.OneOf([A.HorizontalFlip(p=1),\n",
        "                     A.RandomRotate90(p=1),\n",
        "                     A.VerticalFlip(p=1)            \n",
        "            ], p=0.5),\n",
        "            A.OneOf([A.MotionBlur(p=1),\n",
        "                     A.GaussNoise(p=1)                 \n",
        "            ], p=0.5),\n",
        "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ],  bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
        "            keypoint_params=A.KeypointParams(format='xy')),\n",
        "    \n",
        "    'val':\n",
        "        A.Compose([\n",
        "            A.Resize(224, 224, always_apply=True),\n",
        "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ], keypoint_params=A.KeypointParams(format='xy')),\n",
        "    \n",
        "    'test':\n",
        "        A.Compose([\n",
        "            A.Resize(224, 224, always_apply=True),\n",
        "          \n",
        "          #  A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D63y3IAbAxB9"
      },
      "source": [
        "dataset = KeypointDataset(data_dir = os.path.join(root_dir,train_dir),label_path = os.path.join(root_dir,train_df_csv) ,transforms=A_transforms,phase=\"train\")\n",
        "train_loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True,num_workers=2, collate_fn=collate_fn)\n",
        "    \n",
        "test_data = TestDataset(os.path.join(root_dir,test_dir), test_imgs,transforms=A_transforms,  phase='test')\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNsbNzxF9FzK"
      },
      "source": [
        "# 4. 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFEQIr04BUkV"
      },
      "source": [
        "model = get_model()\n",
        "model.cuda()\n",
        "set_parameter_requires_grad(model,feature_extracting)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
        "#patience만큼 loss가 향상되지 않으면 learning_rate에 factor을 곱해줌 \n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1, patience = 5, verbose=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngd02azm9V3N"
      },
      "source": [
        "## 4.1 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OGzi1fg0qS7"
      },
      "source": [
        "# 4.1 model load\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcbyAMmJHTb5"
      },
      "source": [
        "# 5. train / save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtbPT0hgBb4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "296975022c364d0386eac1a988ce1bb4",
            "41a515e97546457b9f9ad0b2910fec41",
            "33c03d5d1ced418497d542b4abd6c640",
            "312bee186e6046c0ac827ebcea9bc9b7",
            "cd262bbd15ea45608193c4761abe163a",
            "e46db937f6104628b93f71d087abbe14",
            "680a6fd385bc4704b6b6566cbe0f741e",
            "1cd84fc40526473bb2bd19807e56f316"
          ]
        },
        "outputId": "49ab4bab-3da1-430d-f75d-56dd146f7484"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    min_loss = 9999\n",
        "    loop = tqdm(train_loader)\n",
        "    for i, (images, targets) in enumerate(loop):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        optimizer.zero_grad()\n",
        "        #loss = criterion(model(images), targets)\n",
        "        loss = model(images,targets)['loss_keypoint']\n",
        "        losses.append(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'| epoch: {epoch} | loss: {loss.item():.4f}')\n",
        "            print()\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    if mean_loss < min_loss:\n",
        "      min_loss = mean_loss\n",
        "      torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': loss}\n",
        "              ,os.path.join(root_dir,'data/best_model.pt'))\n",
        "## 7 7 6"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "296975022c364d0386eac1a988ce1bb4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1049.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4192e3ab87e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#loss = criterion(model(images), targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_keypoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 148\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 148\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCEUFVH-HyY"
      },
      "source": [
        "# 6. test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC9Nh_cM9QP5"
      },
      "source": [
        "#추론\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "files = []\n",
        "with torch.no_grad():\n",
        "  loop = tqdm(test_loader)\n",
        "  for filenames, inputs in loop:\n",
        "    pred = model(inputs.to(device))\n",
        "    # x means pred[0],pred[1],-----,pred[batch]\n",
        "    predictions = [x['keypoints'][0][:,:2].reshape(-1).detach().cpu().numpy() for x in pred]\n",
        "    files.extend(filenames)\n",
        "    for prediction in predictions:\n",
        "      all_predictions.append(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbTm7hKI-chM"
      },
      "source": [
        "# 7. 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9q5r82m19Lf"
      },
      "source": [
        "all_predictions = np.array(all_predictions)\n",
        "for i in range(all_predictions.shape[0]):\n",
        "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= 300 / 1920\n",
        "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= 150 / 1080\n",
        "df_sub = pd.read_csv(os.path.join(root_dir,'data/sample_submission.csv'))\n",
        "df = pd.DataFrame(columns=df_sub.columns)\n",
        "df['image'] = files\n",
        "df.iloc[:, 1:] = all_predictions\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhWB90MNAXeA"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "timeday = str(now)[5:10]\n",
        "df.to_csv(os.path.join(root_dir,f'data/submission_{timeday}.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87HPtH5u283R"
      },
      "source": [
        "# train 데이터 확인\n",
        "train_image , target = dataset.__getitem__(0)\n",
        "np.array(train_image).shape\n",
        "target_array = np.array(target['keypoints'][0][:,:2])\n",
        "plt.imshow(np.array(train_image).transpose(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M8aSeedlH9M"
      },
      "source": [
        "#test 확인\n",
        "filename, test_img = test_data.__getitem__(0)\n",
        "plt.imshow(np.array(test_img).transpose(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYbQroCoYa90"
      },
      "source": [
        "# def draw_keypoints(\n",
        "#     image: np.ndarray,\n",
        "#     keypoints: np.ndarray,\n",
        "#     edges: List[Tuple[int, int]] = None,\n",
        "#     keypoint_names: Dict[int, str] = None, \n",
        "#     boxes: bool = True,\n",
        "#     dpi: int = 200\n",
        "# ) -> None:\n",
        "#     \"\"\"\n",
        "#     Args:\n",
        "#         image (ndarray): [H, W, C]\n",
        "#         keypoints (ndarray): [N, 3]\n",
        "#         edges (List(Tuple(int, int))): \n",
        "#     \"\"\"\n",
        "#     np.random.seed(42)\n",
        "#     colors = {k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\n",
        "\n",
        "#     if boxes:\n",
        "#         x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
        "#         x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
        "#         cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n",
        "\n",
        "#     for i, keypoint in enumerate(keypoints):\n",
        "#         cv2.circle(\n",
        "#             image, \n",
        "#             tuple(keypoint), \n",
        "#             3,(255,0,0), thickness=3, lineType=cv2.FILLED)\n",
        "\n",
        "#         if keypoint_names is not None:\n",
        "#             cv2.putText(\n",
        "#                 image, \n",
        "#                 f'{i}: {keypoint_names[i]}', \n",
        "#                 tuple(keypoint), \n",
        "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "#     if edges is not None:\n",
        "#         for i, edge in enumerate(edges):\n",
        "#             cv2.line(\n",
        "#                 image, \n",
        "#                 tuple(keypoints[edge[0]]), \n",
        "#                 tuple(keypoints[edge[1]]),\n",
        "#                 colors.get(edge[0]), 3, lineType=cv2.LINE_AA)\n",
        "\n",
        "#     fig, ax = plt.subplots(dpi=dpi)\n",
        "#     ax.imshow(image)\n",
        "#     ax.axis('off')\n",
        "#     plt.show()\n",
        "#     keypoints = target_array\n",
        "# keypoint_names = {\n",
        "#     0: 'nose',\n",
        "#     1: 'left_eye',\n",
        "#     2: 'right_eye',\n",
        "#     3: 'left_ear', \n",
        "#     4: 'right_ear', \n",
        "#     5: 'left_shoulder', \n",
        "#     6: 'right_shoulder',\n",
        "#     7: 'left_elbow', \n",
        "#     8: 'right_elbow',\n",
        "#     9: 'left_wrist', \n",
        "#     10: 'right_wrist',\n",
        "#     11: 'left_hip', \n",
        "#     12: 'right_hip',\n",
        "#     13: 'left_knee', \n",
        "#     14: 'right_knee',\n",
        "#     15: 'left_ankle', \n",
        "#     16: 'right_ankle',\n",
        "#     17: 'neck', \n",
        "#     18: 'left_palm', \n",
        "#     19: 'right_palm', \n",
        "#     20: 'spine2(back)',\n",
        "#     21: 'spine1(waist)', \n",
        "#     22: 'left_instep',\n",
        "#     23: 'right_instep'\n",
        "# }\n",
        "\n",
        "# edges = [\n",
        "#     (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (9, 18),\n",
        "#     (10, 19), (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\n",
        "#     (14, 16), (15, 22), (16, 23), (20, 21), (5, 6), (5, 11),\n",
        "#     (6, 12), (11, 12), (17, 20), (20, 21), \n",
        "# ]\n",
        "# draw_keypoints(np.array(train_image).transpose(1,2,0), keypoints, edges, keypoint_names, boxes=False, dpi=400)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}