{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNPYQE2r7oDJ"
   },
   "source": [
    "#1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install albumentations==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvKRcPXmFbjW",
    "outputId": "c67d5cff-7a4f-4079-96d3-b0d0f4b64309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Sequence, Callable, Dict\n",
    "from torch.utils.tensorboard import SummaryWriter #tensorboard\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu 사용량 : {0: 179}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    \"\"\"Get the current gpu usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage: dict\n",
    "        Keys are device ids as integers.\n",
    "        Values are memory usage as integers in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ], encoding='utf-8')\n",
    "    # Convert lines into a dictionary\n",
    "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "    return gpu_memory_map\n",
    "\n",
    "#gpu 메모리 확인\n",
    "print(f'gpu 사용량 : {get_gpu_memory_map()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "55SCipZKD4Av"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPzwS5ky8Qbw"
   },
   "source": [
    "# 2. 코랩 연결 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jsrq745eAy5k",
    "outputId": "f0973e5f-e543-4a1a-fd8e-6f110774f428"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a145c0899d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_dir = '/content/drive/MyDrive/'\n",
    "root_dir = '/home/jngeun/dacon/motion_keypoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IPPO8rFTANWs"
   },
   "outputs": [],
   "source": [
    "feature_extracting= False\n",
    "num_classes = 48\n",
    "learning_rate = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 1000\n",
    "test_dir = 'data/test_imgs'\n",
    "train_dir = \"data/train_imgs\"\n",
    "train_df_csv = \"data/train_df.csv\"\n",
    "test_imgs = os.listdir(os.path.join(root_dir,test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzW2W51j8ce2"
   },
   "source": [
    "# 3. 함수 정의 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3w_UC1rXSrR7"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EpgIox1NAtz4"
   },
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_path, phase, transforms):\n",
    "        self.data_dir = data_dir\n",
    "        self.df = pd.read_csv(label_path)\n",
    "        self.transforms = transforms\n",
    "        self.phase= phase\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[Tensor, Dict]:\n",
    "        image_id = self.df.iloc[index, 0]\n",
    "        labels = np.array([1])\n",
    "        keypoints = self.df.iloc[index, 1:].values.reshape(-1, 2).astype(np.int64)\n",
    "\n",
    "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
    "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
    "        boxes = np.array([[x1, y1, x2, y2]], dtype=np.int64)\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_id), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        targets ={\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels,\n",
    "            'keypoints': keypoints\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "          targets = self.transforms[self.phase](**targets)\n",
    "        \n",
    "        image = targets['image']\n",
    "        image = image / 255.0\n",
    "\n",
    "        targets = {\n",
    "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
    "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
    "            'keypoints': torch.as_tensor(\n",
    "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
    "            )\n",
    "        }\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3k10v5KC_5IJ"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, phase, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.imgs[idx]\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms[self.phase](image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        img = img / 255.0\n",
    "        return filename, img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RENpSKfaBfRp"
   },
   "outputs": [],
   "source": [
    "def get_model() -> nn.Module:\n",
    "    backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    keypoint_roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=14,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = KeypointRCNN(\n",
    "        backbone, \n",
    "        num_classes=2,\n",
    "        num_keypoints=24,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        keypoint_roi_pool=keypoint_roi_pooler\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ipxM55g2Duq0"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model,feature_extracting):\n",
    "  if feature_extracting:\n",
    "    for param in model.backbone.parameters():\n",
    "      param.requires_grad = False\n",
    "      # False로 바뀐 부분을 학습 안하겠다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gB8TRdM0P785"
   },
   "outputs": [],
   "source": [
    "# augmentation \n",
    "A_transforms = {\n",
    "    'train':\n",
    "        A.Compose([\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.Rotate(limit=40,p=0.9),\n",
    "            A.OneOf([A.HorizontalFlip(p=1),\n",
    "                     A.RandomRotate90(p=1),\n",
    "                     A.VerticalFlip(p=1)            \n",
    "            ], p=0.5),\n",
    "            A.OneOf([A.MotionBlur(p=1),\n",
    "                     A.GaussNoise(p=1)                 \n",
    "            ], p=0.5),\n",
    "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ],  bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            keypoint_params=A.KeypointParams(format='xy')),\n",
    "    \n",
    "    'val':\n",
    "        A.Compose([\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            keypoint_params=A.KeypointParams(format='xy')),\n",
    "    \n",
    "    'test':\n",
    "        A.Compose([\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "          \n",
    "          #  A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "D63y3IAbAxB9"
   },
   "outputs": [],
   "source": [
    "#train, validation 나누기\n",
    "train_data = KeypointDataset(data_dir = os.path.join(root_dir,train_dir),label_path = \n",
    "                          os.path.join(root_dir,train_df_csv) ,transforms=A_transforms,phase=\"train\")\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=2, collate_fn=collate_fn)\n",
    "    \n",
    "test_data = TestDataset(os.path.join(root_dir,test_dir), test_imgs,transforms=A_transforms,  phase='test')\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNsbNzxF9FzK"
   },
   "source": [
    "# 4. 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uFEQIr04BUkV"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.cuda()\n",
    "set_parameter_requires_grad(model,feature_extracting)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "#patience만큼 loss가 향상되지 않으면 learning_rate에 factor을 곱해줌 \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1, patience = 5, verbose=True)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1419}\n"
     ]
    }
   ],
   "source": [
    "#gpu 메모리 확인\n",
    "print(get_gpu_memory_map())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngd02azm9V3N"
   },
   "source": [
    "## 4.1 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2OGzi1fg0qS7"
   },
   "outputs": [],
   "source": [
    "# 4.1 model load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcbyAMmJHTb5"
   },
   "source": [
    "# 5. train / save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "296975022c364d0386eac1a988ce1bb4",
      "41a515e97546457b9f9ad0b2910fec41",
      "33c03d5d1ced418497d542b4abd6c640",
      "312bee186e6046c0ac827ebcea9bc9b7",
      "cd262bbd15ea45608193c4761abe163a",
      "e46db937f6104628b93f71d087abbe14",
      "680a6fd385bc4704b6b6566cbe0f741e",
      "1cd84fc40526473bb2bd19807e56f316"
     ]
    },
    "id": "KtbPT0hgBb4x",
    "outputId": "49ab4bab-3da1-430d-f75d-56dd146f7484"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e899916915c4648a9998c1663bccc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 0 | loss: 8.0532 | lr : 0.0001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3ee5685eb64e0ca786090171821dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 1 | loss: 8.0055 | lr : 0.0001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9885829028b3406a886a69f8cc865336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 2 | loss: 7.8909 | lr : 0.0001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e1d2e8e0223a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m               \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m               'loss': loss}\n\u001b[0;32m---> 34\u001b[0;31m               ,os.path.join(root_dir,'data/best_model.pt'))\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_loss = 9999\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    loop = tqdm(train_loader)\n",
    "    for i, (images, targets) in enumerate(loop):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        #loss = criterion(model(images), targets)\n",
    "        loss = model(images,targets)['loss_keypoint']\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(mean_loss)\n",
    "    writer.add_scalar(\"Loss/train\", mean_loss, epoch)\n",
    "    writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "    print(f'| epoch: {epoch} | loss: {mean_loss:.4f} | lr : {lr}')\n",
    "    print()\n",
    "        \n",
    "    if mean_loss < min_loss:\n",
    "      min_loss = mean_loss\n",
    "      torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss}\n",
    "              ,os.path.join(root_dir,'model/best_model.pt'))\n",
    "        \n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\n",
      "TensorBoard 2.4.0 at http://localhost:6007/ (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nCEUFVH-HyY"
   },
   "source": [
    "# 6. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZC9Nh_cM9QP5"
   },
   "outputs": [],
   "source": [
    "#추론\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "files = []\n",
    "with torch.no_grad():\n",
    "  loop = tqdm(test_loader)\n",
    "  for filenames, inputs in loop:\n",
    "    pred = model(inputs.to(device))\n",
    "    # x means pred[0],pred[1],-----,pred[batch]\n",
    "    predictions = [x['keypoints'][0][:,:2].reshape(-1).detach().cpu().numpy() for x in pred]\n",
    "    files.extend(filenames)\n",
    "    for prediction in predictions:\n",
    "      all_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbTm7hKI-chM"
   },
   "source": [
    "# 7. 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9q5r82m19Lf"
   },
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= 300 / 1920\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= 150 / 1080\n",
    "df_sub = pd.read_csv(os.path.join(root_dir,'data/sample_submission.csv'))\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df['image'] = files\n",
    "df.iloc[:, 1:] = all_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhWB90MNAXeA"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timeday = str(now)[5:10]\n",
    "df.to_csv(os.path.join(root_dir,f'data/submission_{timeday}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87HPtH5u283R"
   },
   "outputs": [],
   "source": [
    "# train 데이터 확인\n",
    "train_image , target = train_data.__getitem__(0)\n",
    "np.array(train_image).shape\n",
    "target_array = np.array(target['keypoints'][0][:,:2])\n",
    "plt.imshow(np.array(train_image).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M8aSeedlH9M"
   },
   "outputs": [],
   "source": [
    "#test 확인\n",
    "filename, test_img = test_data.__getitem__(0)\n",
    "plt.imshow(np.array(test_img).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYbQroCoYa90"
   },
   "outputs": [],
   "source": [
    "# def draw_keypoints(\n",
    "#     image: np.ndarray,\n",
    "#     keypoints: np.ndarray,\n",
    "#     edges: List[Tuple[int, int]] = None,\n",
    "#     keypoint_names: Dict[int, str] = None, \n",
    "#     boxes: bool = True,\n",
    "#     dpi: int = 200\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         image (ndarray): [H, W, C]\n",
    "#         keypoints (ndarray): [N, 3]\n",
    "#         edges (List(Tuple(int, int))): \n",
    "#     \"\"\"\n",
    "#     np.random.seed(42)\n",
    "#     colors = {k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\n",
    "\n",
    "#     if boxes:\n",
    "#         x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
    "#         x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
    "#         cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n",
    "\n",
    "#     for i, keypoint in enumerate(keypoints):\n",
    "#         cv2.circle(\n",
    "#             image, \n",
    "#             tuple(keypoint), \n",
    "#             3,(255,0,0), thickness=3, lineType=cv2.FILLED)\n",
    "\n",
    "#         if keypoint_names is not None:\n",
    "#             cv2.putText(\n",
    "#                 image, \n",
    "#                 f'{i}: {keypoint_names[i]}', \n",
    "#                 tuple(keypoint), \n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "#     if edges is not None:\n",
    "#         for i, edge in enumerate(edges):\n",
    "#             cv2.line(\n",
    "#                 image, \n",
    "#                 tuple(keypoints[edge[0]]), \n",
    "#                 tuple(keypoints[edge[1]]),\n",
    "#                 colors.get(edge[0]), 3, lineType=cv2.LINE_AA)\n",
    "\n",
    "#     fig, ax = plt.subplots(dpi=dpi)\n",
    "#     ax.imshow(image)\n",
    "#     ax.axis('off')\n",
    "#     plt.show()\n",
    "#     keypoints = target_array\n",
    "# keypoint_names = {\n",
    "#     0: 'nose',\n",
    "#     1: 'left_eye',\n",
    "#     2: 'right_eye',\n",
    "#     3: 'left_ear', \n",
    "#     4: 'right_ear', \n",
    "#     5: 'left_shoulder', \n",
    "#     6: 'right_shoulder',\n",
    "#     7: 'left_elbow', \n",
    "#     8: 'right_elbow',\n",
    "#     9: 'left_wrist', \n",
    "#     10: 'right_wrist',\n",
    "#     11: 'left_hip', \n",
    "#     12: 'right_hip',\n",
    "#     13: 'left_knee', \n",
    "#     14: 'right_knee',\n",
    "#     15: 'left_ankle', \n",
    "#     16: 'right_ankle',\n",
    "#     17: 'neck', \n",
    "#     18: 'left_palm', \n",
    "#     19: 'right_palm', \n",
    "#     20: 'spine2(back)',\n",
    "#     21: 'spine1(waist)', \n",
    "#     22: 'left_instep',\n",
    "#     23: 'right_instep'\n",
    "# }\n",
    "\n",
    "# edges = [\n",
    "#     (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (9, 18),\n",
    "#     (10, 19), (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\n",
    "#     (14, 16), (15, 22), (16, 23), (20, 21), (5, 6), (5, 11),\n",
    "#     (6, 12), (11, 12), (17, 20), (20, 21), \n",
    "# ]\n",
    "# draw_keypoints(np.array(train_image).transpose(1,2,0), keypoints, edges, keypoint_names, boxes=False, dpi=400)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3/19 과제 코드.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1cd84fc40526473bb2bd19807e56f316": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "296975022c364d0386eac1a988ce1bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_33c03d5d1ced418497d542b4abd6c640",
       "IPY_MODEL_312bee186e6046c0ac827ebcea9bc9b7"
      ],
      "layout": "IPY_MODEL_41a515e97546457b9f9ad0b2910fec41"
     }
    },
    "312bee186e6046c0ac827ebcea9bc9b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cd84fc40526473bb2bd19807e56f316",
      "placeholder": "​",
      "style": "IPY_MODEL_680a6fd385bc4704b6b6566cbe0f741e",
      "value": " 2/1049 [00:03&lt;30:43,  1.76s/it]"
     }
    },
    "33c03d5d1ced418497d542b4abd6c640": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e46db937f6104628b93f71d087abbe14",
      "max": 1049,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd262bbd15ea45608193c4761abe163a",
      "value": 2
     }
    },
    "41a515e97546457b9f9ad0b2910fec41": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "680a6fd385bc4704b6b6566cbe0f741e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd262bbd15ea45608193c4761abe163a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e46db937f6104628b93f71d087abbe14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
