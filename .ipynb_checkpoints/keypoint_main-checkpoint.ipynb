{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNPYQE2r7oDJ"
   },
   "source": [
    "#1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKZwHy2laPMw",
    "outputId": "8a8c8c26-d5c9-43f1-9ab0-221b8d42b6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations==0.4.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
      "Collecting imgaug>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 14.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
      "Building wheels for collected packages: albumentations\n",
      "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.6-cp37-none-any.whl size=65163 sha256=59c95fcdb4ed52e62959216ff25ccfc867cb29cd7d168150f76fdbb48663eaac\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
      "Successfully built albumentations\n",
      "Installing collected packages: imgaug, albumentations\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "  Found existing installation: albumentations 0.1.12\n",
      "    Uninstalling albumentations-0.1.12:\n",
      "      Successfully uninstalled albumentations-0.1.12\n",
      "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install albumentations==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvKRcPXmFbjW",
    "outputId": "18872eec-8c55-4129-d712-37e647cd26c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Sequence, Callable, Dict\n",
    "from torch.utils.tensorboard import SummaryWriter #tensorboard\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4y_DfmxaPM1",
    "outputId": "6ee4e2b6-e132-403b-adb1-c14a68b96ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu 사용량 : {0: 321}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    \"\"\"Get the current gpu usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage: dict\n",
    "        Keys are device ids as integers.\n",
    "        Values are memory usage as integers in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ], encoding='utf-8')\n",
    "    # Convert lines into a dictionary\n",
    "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "    return gpu_memory_map\n",
    "\n",
    "#gpu 메모리 확인\n",
    "print(f'gpu 사용량 : {get_gpu_memory_map()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ikz70FfSaPM2"
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55SCipZKD4Av",
    "outputId": "dcae421c-a037-455a-aa09-afce025de796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPzwS5ky8Qbw"
   },
   "source": [
    "# 2. 코랩 연결 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jsrq745eAy5k",
    "outputId": "272df162-ec4e-4676-d025-313c1039e534"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a145c0899d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OCnhYNOuaPM6"
   },
   "outputs": [],
   "source": [
    "#root_dir = '/content/drive/MyDrive/'\n",
    "root_dir = '/home/jngeun/motion_keypoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IPPO8rFTANWs"
   },
   "outputs": [],
   "source": [
    "feature_extracting= False\n",
    "num_classes = 48\n",
    "learning_rate = 1e-4\n",
    "batch_size = 2\n",
    "num_epochs = 1000\n",
    "test_dir = 'data/test_imgs'\n",
    "train_dir = \"data/train_imgs\"\n",
    "train_df_csv = \"data/train_df.csv\"\n",
    "test_imgs = os.listdir(os.path.join(root_dir,test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzW2W51j8ce2"
   },
   "source": [
    "# 3. 함수 정의 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3w_UC1rXSrR7"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RENpSKfaBfRp"
   },
   "outputs": [],
   "source": [
    "def get_model() -> nn.Module:\n",
    "    backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    keypoint_roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=14,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = KeypointRCNN(\n",
    "        backbone, \n",
    "        num_classes=2,\n",
    "        num_keypoints=24,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        keypoint_roi_pool=keypoint_roi_pooler\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ipxM55g2Duq0"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model,feature_extracting):\n",
    "  if feature_extracting:\n",
    "    for param in model.backbone.parameters():\n",
    "      param.requires_grad = False\n",
    "      # False로 바뀐 부분을 학습 안하겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h0TY1avP4W-"
   },
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8u47qZN7aPNH"
   },
   "outputs": [],
   "source": [
    "def gaussian_filter(kernel_shape):\n",
    "    x = np.zeros(kernel_shape, dtype='float32')\n",
    " \n",
    "    def gauss(x, y, sigma=2.0):\n",
    "        Z = 2 * np.pi * sigma ** 2\n",
    "        return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n",
    " \n",
    "    mid = np.floor(kernel_shape[-1] / 2.)\n",
    "    for kernel_idx in range(0, kernel_shape[1]):\n",
    "        for i in range(0, kernel_shape[2]):\n",
    "            for j in range(0, kernel_shape[3]):\n",
    "                x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)\n",
    " \n",
    "    return x / np.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XlzcY-05aPNI"
   },
   "outputs": [],
   "source": [
    "def LocalContrastNorm(image,radius=9):\n",
    "    \"\"\"\n",
    "    image: torch.Tensor , .shape => (1,channels,height,width) \n",
    "    \n",
    "    radius: Gaussian filter size (int), odd\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if radius%2 == 0:\n",
    "        radius += 1\n",
    "    def get_gaussian_filter(kernel_shape):\n",
    "        x = np.zeros(kernel_shape, dtype='float64')\n",
    " \n",
    "        def gauss(x, y, sigma=2.0):\n",
    "            Z = 2 * np.pi * sigma ** 2\n",
    "            return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n",
    " \n",
    "        mid = np.floor(kernel_shape[-1] / 2.)\n",
    "        for kernel_idx in range(0, kernel_shape[1]):\n",
    "            for i in range(0, kernel_shape[2]):\n",
    "                for j in range(0, kernel_shape[3]):\n",
    "                    x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)\n",
    " \n",
    "        return x / np.sum(x)\n",
    "    \n",
    "    n,c,h,w = image.shape[0],image.shape[1],image.shape[2],image.shape[3]\n",
    "\n",
    "    gaussian_filter = torch.Tensor(get_gaussian_filter((1,c,radius,radius))).double()\n",
    "    filtered_out = F.conv2d(image,gaussian_filter,padding=radius-1)\n",
    "    mid = int(np.floor(gaussian_filter.shape[2] / 2.))\n",
    "    ### Subtractive Normalization\n",
    "    centered_image = image - filtered_out[:,:,mid:-mid,mid:-mid]\n",
    "    \n",
    "    ## Variance Calc\n",
    "    sum_sqr_image = F.conv2d(centered_image.pow(2),gaussian_filter,padding=radius-1)\n",
    "    s_deviation = sum_sqr_image[:,:,mid:-mid,mid:-mid].sqrt()\n",
    "    #gfilter = torch.Tensor(gaussian_filter((1,3,9,9))).double()\n",
    "    #filtered = F.conv2d(image_tensor,gfilter,padding=8) ## padding = 8 = 9-1 (radius - 1 ) for border == 'full'\n",
    "    sum_sqr_XX = F.conv2d(centered_image.pow(2),gaussian_filter,padding=8)\n",
    "    denom = sum_sqr_XX[:,:,mid:-mid,mid:-mid].sqrt()\n",
    "    per_img_mean = denom.mean()\n",
    "    \n",
    "    ## Divisive Normalization\n",
    "    divisor = np.maximum(per_img_mean.numpy(),s_deviation.numpy())\n",
    "    divisor = np.maximum(divisor, 1e-4)\n",
    "    new_image = centered_image / torch.Tensor(divisor)\n",
    "    return new_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U1uss4CLUNO"
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRFcsfKcaPM_"
   },
   "source": [
    "## checkout augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iFWimNrmaPNA"
   },
   "outputs": [],
   "source": [
    "def draw_keypoints(\n",
    "    image: np.ndarray,\n",
    "    keypoints: np.ndarray,\n",
    "    edges: List[Tuple[int, int]] = None,\n",
    "    keypoint_names: Dict[int, str] = None, \n",
    "    boxes: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image (ndarray): [H, W, C]\n",
    "        keypoints (ndarray): [N, 3]\n",
    "        edges (List(Tuple(int, int))): \n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    colors = {k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\n",
    "\n",
    "    if boxes:\n",
    "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
    "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n",
    "\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        cv2.circle(\n",
    "            image, \n",
    "            tuple(keypoint), \n",
    "            3, colors.get(i), thickness=1, lineType=cv2.FILLED)\n",
    "\n",
    "        if keypoint_names is not None:\n",
    "            cv2.putText(\n",
    "                image, \n",
    "                f'{i}: {keypoint_names[i]}', \n",
    "                tuple(keypoint), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "    if edges is not None:\n",
    "        for i, edge in enumerate(edges):\n",
    "            cv2.line(\n",
    "                image, \n",
    "                tuple(keypoints[edge[0]]), \n",
    "                tuple(keypoints[edge[1]]),\n",
    "                colors.get(edge[0]), 1, lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0coyyrOBaPNB"
   },
   "outputs": [],
   "source": [
    "def checkout_image(idx : int):\n",
    "    img, targets = train_data.__getitem__(idx)\n",
    "    \n",
    "    img = np.array(img)\n",
    "    keypoints = np.array(targets['keypoints'][:,:,:2]).reshape(-1, 2)\n",
    "    keypoints = keypoints.astype(np.int64)\n",
    "    \n",
    "    keypoint_names = {\n",
    "        0: 'nose',\n",
    "        1: 'left_eye',\n",
    "        2: 'right_eye',\n",
    "        3: 'left_ear', \n",
    "        4: 'right_ear', \n",
    "        5: 'left_shoulder', \n",
    "        6: 'right_shoulder',\n",
    "        7: 'left_elbow', \n",
    "        8: 'right_elbow',\n",
    "        9: 'left_wrist', \n",
    "        10: 'right_wrist',\n",
    "        11: 'left_hip', \n",
    "        12: 'right_hip',\n",
    "        13: 'left_knee', \n",
    "        14: 'right_knee',\n",
    "        15: 'left_ankle', \n",
    "        16: 'right_ankle',\n",
    "        17: 'neck', \n",
    "        18: 'left_palm', \n",
    "        19: 'right_palm', \n",
    "        20: 'spine2(back)',\n",
    "        21: 'spine1(waist)', \n",
    "        22: 'left_instep',\n",
    "        23: 'right_instep'\n",
    "    }\n",
    "\n",
    "    edges = [\n",
    "        (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (9, 18),\n",
    "        (10, 19), (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\n",
    "        (14, 16), (15, 22), (16, 23), (20, 21), (5, 6), (5, 11),\n",
    "        (6, 12), (11, 12), (17, 20), (20, 21), \n",
    "    ]\n",
    "    img = np.transpose(img,(0,1,2))\n",
    "    img =  draw_keypoints(img, keypoints,edges, keypoint_names, boxes=False)\n",
    "  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "iYrV1DuoaPNE",
    "outputId": "afb96193-a140-4f79-fa1c-8ef6db8c7bc1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-03acda4975c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckout_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d8528ad696a6>\u001b[0m in \u001b[0;36mcheckout_image\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheckout_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "pred_img = checkout_image(0)\n",
    "fig, ax = plt.subplots(dpi=224)\n",
    "ax.imshow(pred_img)   \n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXifGaIaL5ZA"
   },
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4FuJzrcNIee"
   },
   "source": [
    "## class 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gB8TRdM0P785"
   },
   "outputs": [],
   "source": [
    "# augmentation \n",
    "A_transforms = {\n",
    "    'train':\n",
    "        A.Compose([\n",
    "            #A.Resize(224, 224, always_apply=True),\n",
    "            A.Rotate(limit=40,p=0),\n",
    "            A.OneOf([A.HorizontalFlip(p=1),\n",
    "                     A.RandomRotate90(p=1),\n",
    "                     A.VerticalFlip(p=1)            \n",
    "            ], p=0),\n",
    "            A.OneOf([A.MotionBlur(p=1),\n",
    "                     A.GaussNoise(p=1)                 \n",
    "            ], p=0),\n",
    "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ],  bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            keypoint_params=A.KeypointParams(format='xy')) ,\n",
    "    \n",
    "    'val':\n",
    "        A.Compose([\n",
    "            #A.Resize(224, 224, always_apply=True),\n",
    "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            keypoint_params=A.KeypointParams(format='xy')),\n",
    "    \n",
    "    'test':\n",
    "        A.Compose([\n",
    "            #A.Resize(224, 224, always_apply=True),\n",
    "          \n",
    "          #  A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EpgIox1NAtz4"
   },
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_path, phase=None, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.df = pd.read_csv(label_path)\n",
    "        self.transforms = transforms\n",
    "        self.phase= phase\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[Tensor, Dict]:\n",
    "        image_id = self.df.iloc[index, 0]\n",
    "        labels = np.array([1])\n",
    "        keypoints = self.df.iloc[index, 1:].values.reshape(-1, 2).astype(np.int64)\n",
    "\n",
    "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
    "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
    "        boxes = np.array([[x1, y1, x2, y2]], dtype=np.int64)\n",
    "\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_id), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        targets ={\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels,\n",
    "            'keypoints': keypoints\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "          targets = self.transforms[self.phase](**targets)\n",
    "        \n",
    "        image = targets['image']\n",
    "        image = image / 255.0\n",
    "\n",
    "        targets = {\n",
    "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
    "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
    "            'keypoints': torch.as_tensor(\n",
    "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
    "            )\n",
    "        }\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3k10v5KC_5IJ"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, phase, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.imgs[idx]\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms[self.phase](image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        img = img / 255.0\n",
    "        return filename, img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RXxnnANYatxE"
   },
   "outputs": [],
   "source": [
    "# train dataset augmentation 을 위한 maping class\n",
    "class MapTrainDataset(Dataset): \n",
    "    \"\"\"\n",
    "    Given a dataset, creates a dataset which applies a mapping function\n",
    "    to its items (lazily, only when an item is called).\n",
    "\n",
    "    Note that data is not cloned/copied from the initial dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      targets ={\n",
    "            'image': self.dataset[index][0],\n",
    "            'bboxes': self.dataset[index][1]['boxes'],\n",
    "            'labels': self.dataset[index][1]['labels'],\n",
    "            'keypoints': self.dataset[index][1]['keypoints'][0][:,:2]\n",
    "        }\n",
    "      targets = A_transforms['train'](**targets)  \n",
    "      image =targets['image']\n",
    "      \n",
    "      #LCN\n",
    "      image = torch.tensor([np.array(image)])\n",
    "      image = LocalContrastNorm(image,radius=9)[0]\n",
    "      scaled_img = (image - torch.min(image))/(torch.max(image) - torch.min(image)) #image를 0~1로 고정\n",
    "\n",
    "      targets = {\n",
    "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
    "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
    "            'keypoints': torch.as_tensor(\n",
    "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
    "            )\n",
    "        }\n",
    "      return scaled_img, targets\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OCDBvRBKauu8"
   },
   "outputs": [],
   "source": [
    "# valid dataset augmentation 을 위한 maping class\n",
    "class MapValidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, creates a dataset which applies a mapping function\n",
    "    to its items (lazily, only when an item is called).\n",
    "\n",
    "    Note that data is not cloned/copied from the initial dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      targets ={\n",
    "            'image': self.dataset[index][0],\n",
    "            'bboxes': self.dataset[index][1]['boxes'],\n",
    "            'labels': self.dataset[index][1]['labels'],\n",
    "            'keypoints': self.dataset[index][1]['keypoints'][0][:,:2]\n",
    "        }\n",
    "      targets = A_transforms['val'](**targets)  \n",
    "      image =targets['image']\n",
    "\n",
    "      #LCN\n",
    "      image = torch.tensor([np.array(image)])\n",
    "      image = LocalContrastNorm(image,radius=9)[0]\n",
    "      scaled_img = (image - torch.min(image))/(torch.max(image) - torch.min(image)) #image를 0~1로 고정\n",
    "\n",
    "      targets = {\n",
    "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
    "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
    "            'keypoints': torch.as_tensor(\n",
    "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
    "            )\n",
    "        }\n",
    "      return scaled_img, targets\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VUTtEMp_azYx"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# 항상 동일한 valid set을 설정\n",
    "\n",
    "train_data = KeypointDataset(data_dir = os.path.join(root_dir,train_dir),label_path = os.path.join(root_dir,train_df_csv))\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.1 * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "tng_data = torch.utils.data.Subset(train_data, train_idx)\n",
    "val_data = torch.utils.data.Subset(train_data, valid_idx)\n",
    "\n",
    "tng_data_tf = MapTrainDataset(tng_data)\n",
    "val_data_tf = MapValidDataset(val_data)\n",
    "# maping for augmentation\n",
    "\n",
    "train_loader = DataLoader(tng_data_tf, batch_size=batch_size,collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data_tf, batch_size=batch_size,collate_fn=collate_fn)\n",
    "# data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFdhSQTHaPNF"
   },
   "source": [
    "## Local Contrast Normalization\n",
    "이론링크 : https://deepestdocs.readthedocs.io/en/latest/003_image_processing/0030/  \n",
    "코드링크 : https://github.com/dibyadas/Visualize-Normalizations/blob/master/LocalContrastNorm.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNsbNzxF9FzK"
   },
   "source": [
    "# 4. 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uFEQIr04BUkV"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.cuda()\n",
    "set_parameter_requires_grad(model,feature_extracting)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "#patience만큼 loss가 향상되지 않으면 learning_rate에 factor을 곱해줌 \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1, patience = 5, verbose=True)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prTc7fDxaPNM",
    "outputId": "1cefa78f-f7f6-4248-9615-29dfb03973fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1496}\n"
     ]
    }
   ],
   "source": [
    "#gpu 메모리 확인\n",
    "print(get_gpu_memory_map())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngd02azm9V3N"
   },
   "source": [
    "## 4.1 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2OGzi1fg0qS7"
   },
   "outputs": [],
   "source": [
    "# 4.1 model load\n",
    "\n",
    "# model = get_model()\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# #optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1, patience = 5, verbose=True)\n",
    "# checkpoint = torch.load('/content/drive/MyDrive/data/best_model_0.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcbyAMmJHTb5"
   },
   "source": [
    "# 5. train / save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3f50301cc73d4fe8b2df1a847e33e16b",
      "7743834c4f9548d291d4fbe0b872b48d",
      "45ce974aa7ed470da7b582e3d9355ad4",
      "24e6a115345840589050dd9b63cf373b",
      "1ce1a4eac2b74c62b06c9a38234f8e09",
      "3a61407e06d74e3ca2cf2b89bd3c2aae",
      "9e46a458c19b4f5c8aab833b4c70d51e",
      "bd810022b3f542cdacf394526333d331"
     ]
    },
    "id": "KtbPT0hgBb4x",
    "outputId": "3dd93e45-7b51-40bd-8832-3e979a80bcab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45381dfbbfd34ab7b1d70ab6813ad999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 0 | loss: 9.4487 | lr : 0.0001 | validation loss : 9.4812\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b686f5c112a04eacb93d531b3b93ff35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_loss = 9999\n",
    "\n",
    "early_stopping_idx = 0;\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    cal_loss = []\n",
    "    loss_for_validation = []\n",
    "    losses = []\n",
    "    loop = tqdm(train_loader)\n",
    "    for i, (images, targets) in enumerate(loop):\n",
    "        images = list(image.float().to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        #loss = criterion(model(images), targets)\n",
    "        losses = model(images,targets)\n",
    "        loss = sum(loss for loss in losses.values())\n",
    "        loss.backward()\n",
    "        cal_loss.append(loss)\n",
    "        if (i+1) % 10 == 0:\n",
    "          print(f'| epoch: {epoch} | loss: {loss.item():.4f}')\n",
    "          print()\n",
    "       \n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "      for k, (val_images, val_targets) in enumerate(val_loader):\n",
    "        val_images = list(val_image.float().to(device) for val_image in val_images)\n",
    "        val_targets = [{k: v.to(device) for k, v in t.items()} for t in val_targets]\n",
    "        val_losses = model(val_images,val_targets)\n",
    "        val_loss = sum(loss for loss in val_losses.values())\n",
    "        loss_for_validation.append(val_loss)\n",
    "\n",
    "\n",
    "    mean_val_loss = sum(loss_for_validation) / len(loss_for_validation)\n",
    "    mean_loss = sum(cal_loss) / len(cal_loss)\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(mean_loss)\n",
    "    writer.add_scalar(\"Loss/val\", mean_val_loss,epoch)\n",
    "    writer.add_scalar(\"Loss/train\", mean_loss, epoch)\n",
    "    writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "    print(f'| epoch: {epoch} | loss: {mean_loss:.4f} | lr : {lr} | validation loss : {mean_val_loss:.4f}')\n",
    "    print()\n",
    "        \n",
    "    if mean_val_loss < min_loss:\n",
    "      early_stopping_idx = 0\n",
    "      min_loss = mean_val_loss\n",
    "      torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss}\n",
    "              ,os.path.join(root_dir,'best_model.pt'))\n",
    "    else:\n",
    "      print(\"Val_loss_increase\")\n",
    "      early_stopping_idx +=1\n",
    "      if early_stopping_idx == 5:\n",
    "        print(\"early stop\")\n",
    "        break\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jthRVnkR7Cmf",
    "outputId": "bd755a97-4bf4-4060-fe71-22a321a92011"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0262, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cal_loss) / len(cal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xclMHSrPExql",
    "outputId": "3e5da034-782c-45d8-eae1-93a789c1ed62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_box_reg': tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_classifier': tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_keypoint': tensor(8.0329, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_objectness': tensor(0.6896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWwzPxw6aPNP"
   },
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eULC1lNaPNP"
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHUx5v3KaPNP",
    "outputId": "a97c9508-2830-4b76-c5e8-a253656ea992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\n",
      "TensorBoard 2.4.0 at http://localhost:6007/ (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nCEUFVH-HyY"
   },
   "source": [
    "# 6. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZC9Nh_cM9QP5"
   },
   "outputs": [],
   "source": [
    "#추론\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "files = []\n",
    "with torch.no_grad():\n",
    "  loop = tqdm(test_loader)\n",
    "  for filenames, inputs in loop:\n",
    "    pred = model(inputs.to(device))\n",
    "    # x means pred[0],pred[1],-----,pred[batch]\n",
    "    predictions = [x['keypoints'][0][:,:2].reshape(-1).detach().cpu().numpy() for x in pred]\n",
    "    files.extend(filenames)\n",
    "    for prediction in predictions:\n",
    "      all_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbTm7hKI-chM"
   },
   "source": [
    "# 7. 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9q5r82m19Lf"
   },
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= 300 / 1920\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= 150 / 1080\n",
    "df_sub = pd.read_csv(os.path.join(root_dir,'data/sample_submission.csv'))\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df['image'] = files\n",
    "df.iloc[:, 1:] = all_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhWB90MNAXeA"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timeday = str(now)[5:10]\n",
    "df.to_csv(os.path.join(root_dir,f'data/submission_{timeday}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87HPtH5u283R"
   },
   "outputs": [],
   "source": [
    "# train 데이터 확인\n",
    "train_image , target = train_data.__getitem__(0)\n",
    "np.array(train_image).shape\n",
    "target_array = np.array(target['keypoints'][0][:,:2])\n",
    "plt.imshow(np.array(train_image).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M8aSeedlH9M"
   },
   "outputs": [],
   "source": [
    "#test 확인\n",
    "filename, test_img = test_data.__getitem__(0)\n",
    "plt.imshow(np.array(test_img).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYbQroCoYa90"
   },
   "outputs": [],
   "source": [
    "# def draw_keypoints(\n",
    "#     image: np.ndarray,\n",
    "#     keypoints: np.ndarray,\n",
    "#     edges: List[Tuple[int, int]] = None,\n",
    "#     keypoint_names: Dict[int, str] = None, \n",
    "#     boxes: bool = True,\n",
    "#     dpi: int = 200\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         image (ndarray): [H, W, C]\n",
    "#         keypoints (ndarray): [N, 3]\n",
    "#         edges (List(Tuple(int, int))): \n",
    "#     \"\"\"\n",
    "#     np.random.seed(42)\n",
    "#     colors = {k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\n",
    "\n",
    "#     if boxes:\n",
    "#         x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
    "#         x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
    "#         cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n",
    "\n",
    "#     for i, keypoint in enumerate(keypoints):\n",
    "#         cv2.circle(\n",
    "#             image, \n",
    "#             tuple(keypoint), \n",
    "#             3,(255,0,0), thickness=3, lineType=cv2.FILLED)\n",
    "\n",
    "#         if keypoint_names is not None:\n",
    "#             cv2.putText(\n",
    "#                 image, \n",
    "#                 f'{i}: {keypoint_names[i]}', \n",
    "#                 tuple(keypoint), \n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "#     if edges is not None:\n",
    "#         for i, edge in enumerate(edges):\n",
    "#             cv2.line(\n",
    "#                 image, \n",
    "#                 tuple(keypoints[edge[0]]), \n",
    "#                 tuple(keypoints[edge[1]]),\n",
    "#                 colors.get(edge[0]), 3, lineType=cv2.LINE_AA)\n",
    "\n",
    "#     fig, ax = plt.subplots(dpi=dpi)\n",
    "#     ax.imshow(image)\n",
    "#     ax.axis('off')\n",
    "#     plt.show()\n",
    "#     keypoints = target_array\n",
    "# keypoint_names = {\n",
    "#     0: 'nose',\n",
    "#     1: 'left_eye',\n",
    "#     2: 'right_eye',\n",
    "#     3: 'left_ear', \n",
    "#     4: 'right_ear', \n",
    "#     5: 'left_shoulder', \n",
    "#     6: 'right_shoulder',\n",
    "#     7: 'left_elbow', \n",
    "#     8: 'right_elbow',\n",
    "#     9: 'left_wrist', \n",
    "#     10: 'right_wrist',\n",
    "#     11: 'left_hip', \n",
    "#     12: 'right_hip',\n",
    "#     13: 'left_knee', \n",
    "#     14: 'right_knee',\n",
    "#     15: 'left_ankle', \n",
    "#     16: 'right_ankle',\n",
    "#     17: 'neck', \n",
    "#     18: 'left_palm', \n",
    "#     19: 'right_palm', \n",
    "#     20: 'spine2(back)',\n",
    "#     21: 'spine1(waist)', \n",
    "#     22: 'left_instep',\n",
    "#     23: 'right_instep'\n",
    "# }\n",
    "\n",
    "# edges = [\n",
    "#     (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (9, 18),\n",
    "#     (10, 19), (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\n",
    "#     (14, 16), (15, 22), (16, 23), (20, 21), (5, 6), (5, 11),\n",
    "#     (6, 12), (11, 12), (17, 20), (20, 21), \n",
    "# ]\n",
    "# draw_keypoints(np.array(train_image).transpose(1,2,0), keypoints, edges, keypoint_names, boxes=False, dpi=400)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ngd02azm9V3N",
    "OWwzPxw6aPNP"
   ],
   "name": "keypoint_main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ce1a4eac2b74c62b06c9a38234f8e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "24e6a115345840589050dd9b63cf373b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd810022b3f542cdacf394526333d331",
      "placeholder": "​",
      "style": "IPY_MODEL_9e46a458c19b4f5c8aab833b4c70d51e",
      "value": " 472/472 [43:08&lt;00:00,  5.48s/it]"
     }
    },
    "3a61407e06d74e3ca2cf2b89bd3c2aae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f50301cc73d4fe8b2df1a847e33e16b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45ce974aa7ed470da7b582e3d9355ad4",
       "IPY_MODEL_24e6a115345840589050dd9b63cf373b"
      ],
      "layout": "IPY_MODEL_7743834c4f9548d291d4fbe0b872b48d"
     }
    },
    "45ce974aa7ed470da7b582e3d9355ad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a61407e06d74e3ca2cf2b89bd3c2aae",
      "max": 472,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ce1a4eac2b74c62b06c9a38234f8e09",
      "value": 472
     }
    },
    "7743834c4f9548d291d4fbe0b872b48d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e46a458c19b4f5c8aab833b4c70d51e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd810022b3f542cdacf394526333d331": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
