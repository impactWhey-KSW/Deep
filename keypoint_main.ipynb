{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keypoint_main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ngd02azm9V3N",
        "OWwzPxw6aPNP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f50301cc73d4fe8b2df1a847e33e16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7743834c4f9548d291d4fbe0b872b48d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_45ce974aa7ed470da7b582e3d9355ad4",
              "IPY_MODEL_24e6a115345840589050dd9b63cf373b"
            ]
          }
        },
        "7743834c4f9548d291d4fbe0b872b48d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45ce974aa7ed470da7b582e3d9355ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ce1a4eac2b74c62b06c9a38234f8e09",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 472,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 472,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a61407e06d74e3ca2cf2b89bd3c2aae"
          }
        },
        "24e6a115345840589050dd9b63cf373b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e46a458c19b4f5c8aab833b4c70d51e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 472/472 [43:08&lt;00:00,  5.48s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd810022b3f542cdacf394526333d331"
          }
        },
        "1ce1a4eac2b74c62b06c9a38234f8e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a61407e06d74e3ca2cf2b89bd3c2aae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e46a458c19b4f5c8aab833b4c70d51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd810022b3f542cdacf394526333d331": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNPYQE2r7oDJ"
      },
      "source": [
        "#1. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKZwHy2laPMw",
        "outputId": "8a8c8c26-d5c9-43f1-9ab0-221b8d42b6ab"
      },
      "source": [
        "! pip install albumentations==0.4.6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-cp37-none-any.whl size=65163 sha256=59c95fcdb4ed52e62959216ff25ccfc867cb29cd7d168150f76fdbb48663eaac\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvKRcPXmFbjW",
        "outputId": "18872eec-8c55-4129-d712-37e647cd26c1"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "import torchvision\n",
        "print(torchvision.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torchvision.models.detection import KeypointRCNN\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Tuple, List, Sequence, Callable, Dict\n",
        "from torch.utils.tensorboard import SummaryWriter #tensorboard\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.0+cu101\n",
            "0.9.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4y_DfmxaPM1",
        "outputId": "6ee4e2b6-e132-403b-adb1-c14a68b96ced"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map\n",
        "\n",
        "#gpu 메모리 확인\n",
        "print(f'gpu 사용량 : {get_gpu_memory_map()}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu 사용량 : {0: 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikz70FfSaPM2"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55SCipZKD4Av",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcae421c-a037-455a-aa09-afce025de796"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPzwS5ky8Qbw"
      },
      "source": [
        "# 2. 코랩 연결 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsrq745eAy5k",
        "outputId": "272df162-ec4e-4676-d025-313c1039e534"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCnhYNOuaPM6"
      },
      "source": [
        "root_dir = '/content/drive/MyDrive/'\n",
        "#root_dir = '/home/jngeun/motion_keypoint'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPPO8rFTANWs"
      },
      "source": [
        "feature_extracting= False\n",
        "num_classes = 48\n",
        "learning_rate = 1e-4\n",
        "batch_size = 8\n",
        "num_epochs = 1000\n",
        "test_dir = 'data/test_imgs'\n",
        "train_dir = \"data/train_imgs\"\n",
        "train_df_csv = \"data/train_df.csv\"\n",
        "test_imgs = os.listdir(os.path.join(root_dir,test_dir))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzW2W51j8ce2"
      },
      "source": [
        "# 3. 함수 정의 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w_UC1rXSrR7"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RENpSKfaBfRp"
      },
      "source": [
        "def get_model() -> nn.Module:\n",
        "    backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0', '1', '2', '3'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    keypoint_roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0', '1', '2', '3'],\n",
        "        output_size=14,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    model = KeypointRCNN(\n",
        "        backbone, \n",
        "        num_classes=2,\n",
        "        num_keypoints=24,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        keypoint_roi_pool=keypoint_roi_pooler\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipxM55g2Duq0"
      },
      "source": [
        "def set_parameter_requires_grad(model,feature_extracting):\n",
        "  if feature_extracting:\n",
        "    for param in model.backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "      # False로 바뀐 부분을 학습 안하겠다.\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0TY1avP4W-"
      },
      "source": [
        "## image processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u47qZN7aPNH"
      },
      "source": [
        "def gaussian_filter(kernel_shape):\n",
        "    x = np.zeros(kernel_shape, dtype='float32')\n",
        " \n",
        "    def gauss(x, y, sigma=2.0):\n",
        "        Z = 2 * np.pi * sigma ** 2\n",
        "        return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n",
        " \n",
        "    mid = np.floor(kernel_shape[-1] / 2.)\n",
        "    for kernel_idx in range(0, kernel_shape[1]):\n",
        "        for i in range(0, kernel_shape[2]):\n",
        "            for j in range(0, kernel_shape[3]):\n",
        "                x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)\n",
        " \n",
        "    return x / np.sum(x)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlzcY-05aPNI"
      },
      "source": [
        "def LocalContrastNorm(image,radius=9):\n",
        "    \"\"\"\n",
        "    image: torch.Tensor , .shape => (1,channels,height,width) \n",
        "    \n",
        "    radius: Gaussian filter size (int), odd\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    if radius%2 == 0:\n",
        "        radius += 1\n",
        "    def get_gaussian_filter(kernel_shape):\n",
        "        x = np.zeros(kernel_shape, dtype='float64')\n",
        " \n",
        "        def gauss(x, y, sigma=2.0):\n",
        "            Z = 2 * np.pi * sigma ** 2\n",
        "            return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n",
        " \n",
        "        mid = np.floor(kernel_shape[-1] / 2.)\n",
        "        for kernel_idx in range(0, kernel_shape[1]):\n",
        "            for i in range(0, kernel_shape[2]):\n",
        "                for j in range(0, kernel_shape[3]):\n",
        "                    x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)\n",
        " \n",
        "        return x / np.sum(x)\n",
        "    \n",
        "    n,c,h,w = image.shape[0],image.shape[1],image.shape[2],image.shape[3]\n",
        "\n",
        "    gaussian_filter = torch.Tensor(get_gaussian_filter((1,c,radius,radius))).double()\n",
        "    print(image.type())\n",
        "    print(gaussian_filter.type())\n",
        "    filtered_out = F.conv2d(image,gaussian_filter,padding=radius-1)\n",
        "    mid = int(np.floor(gaussian_filter.shape[2] / 2.))\n",
        "    ### Subtractive Normalization\n",
        "    centered_image = image - filtered_out[:,:,mid:-mid,mid:-mid]\n",
        "    \n",
        "    ## Variance Calc\n",
        "    sum_sqr_image = F.conv2d(centered_image.pow(2),gaussian_filter,padding=radius-1)\n",
        "    s_deviation = sum_sqr_image[:,:,mid:-mid,mid:-mid].sqrt()\n",
        "    #gfilter = torch.Tensor(gaussian_filter((1,3,9,9))).double()\n",
        "    #filtered = F.conv2d(image_tensor,gfilter,padding=8) ## padding = 8 = 9-1 (radius - 1 ) for border == 'full'\n",
        "    sum_sqr_XX = F.conv2d(centered_image.pow(2),gaussian_filter,padding=8)\n",
        "    denom = sum_sqr_XX[:,:,mid:-mid,mid:-mid].sqrt()\n",
        "    per_img_mean = denom.mean()\n",
        "    \n",
        "    ## Divisive Normalization\n",
        "    divisor = np.maximum(per_img_mean.numpy(),s_deviation.numpy())\n",
        "    divisor = np.maximum(divisor, 1e-4)\n",
        "    new_image = centered_image / torch.Tensor(divisor)\n",
        "    return new_image\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U1uss4CLUNO"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRFcsfKcaPM_"
      },
      "source": [
        "## checkout augmented image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFWimNrmaPNA"
      },
      "source": [
        "def draw_keypoints(\n",
        "    image: np.ndarray,\n",
        "    keypoints: np.ndarray,\n",
        "    edges: List[Tuple[int, int]] = None,\n",
        "    keypoint_names: Dict[int, str] = None, \n",
        "    boxes: bool = True\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        image (ndarray): [H, W, C]\n",
        "        keypoints (ndarray): [N, 3]\n",
        "        edges (List(Tuple(int, int))): \n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    colors = {k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\n",
        "\n",
        "    if boxes:\n",
        "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
        "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n",
        "\n",
        "    for i, keypoint in enumerate(keypoints):\n",
        "        cv2.circle(\n",
        "            image, \n",
        "            tuple(keypoint), \n",
        "            3, colors.get(i), thickness=1, lineType=cv2.FILLED)\n",
        "\n",
        "        if keypoint_names is not None:\n",
        "            cv2.putText(\n",
        "                image, \n",
        "                f'{i}: {keypoint_names[i]}', \n",
        "                tuple(keypoint), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "    if edges is not None:\n",
        "        for i, edge in enumerate(edges):\n",
        "            cv2.line(\n",
        "                image, \n",
        "                tuple(keypoints[edge[0]]), \n",
        "                tuple(keypoints[edge[1]]),\n",
        "                colors.get(edge[0]), 1, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0coyyrOBaPNB"
      },
      "source": [
        "def checkout_image(idx : int):\n",
        "    img, targets = train_data.__getitem__(idx)\n",
        "    \n",
        "    img = np.array(img)\n",
        "    keypoints = np.array(targets['keypoints'][:,:,:2]).reshape(-1, 2)\n",
        "    keypoints = keypoints.astype(np.int64)\n",
        "    \n",
        "    keypoint_names = {\n",
        "        0: 'nose',\n",
        "        1: 'left_eye',\n",
        "        2: 'right_eye',\n",
        "        3: 'left_ear', \n",
        "        4: 'right_ear', \n",
        "        5: 'left_shoulder', \n",
        "        6: 'right_shoulder',\n",
        "        7: 'left_elbow', \n",
        "        8: 'right_elbow',\n",
        "        9: 'left_wrist', \n",
        "        10: 'right_wrist',\n",
        "        11: 'left_hip', \n",
        "        12: 'right_hip',\n",
        "        13: 'left_knee', \n",
        "        14: 'right_knee',\n",
        "        15: 'left_ankle', \n",
        "        16: 'right_ankle',\n",
        "        17: 'neck', \n",
        "        18: 'left_palm', \n",
        "        19: 'right_palm', \n",
        "        20: 'spine2(back)',\n",
        "        21: 'spine1(waist)', \n",
        "        22: 'left_instep',\n",
        "        23: 'right_instep'\n",
        "    }\n",
        "\n",
        "    edges = [\n",
        "        (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (9, 18),\n",
        "        (10, 19), (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\n",
        "        (14, 16), (15, 22), (16, 23), (20, 21), (5, 6), (5, 11),\n",
        "        (6, 12), (11, 12), (17, 20), (20, 21), \n",
        "    ]\n",
        "    img = np.transpose(img,(0,1,2))\n",
        "    img =  draw_keypoints(img, keypoints,edges, keypoint_names, boxes=False)\n",
        "  \n",
        "    return img"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "iYrV1DuoaPNE",
        "outputId": "afb96193-a140-4f79-fa1c-8ef6db8c7bc1"
      },
      "source": [
        "pred_img = checkout_image(0)\n",
        "fig, ax = plt.subplots(dpi=224)\n",
        "ax.imshow(pred_img)   \n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-03acda4975c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckout_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d8528ad696a6>\u001b[0m in \u001b[0;36mcheckout_image\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheckout_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXifGaIaL5ZA"
      },
      "source": [
        "# 데이터 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4FuJzrcNIee"
      },
      "source": [
        "## class 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpgIox1NAtz4"
      },
      "source": [
        "class KeypointDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_path, phase=None, transforms=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.df = pd.read_csv(label_path)\n",
        "        self.transforms = transforms\n",
        "        self.phase= phase\n",
        "    def __len__(self) -> int:\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index) -> Tuple[Tensor, Dict]:\n",
        "        image_id = self.df.iloc[index, 0]\n",
        "        labels = np.array([1])\n",
        "        keypoints = self.df.iloc[index, 1:].values.reshape(-1, 2).astype(np.int64)\n",
        "\n",
        "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
        "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
        "        boxes = np.array([[x1, y1, x2, y2]], dtype=np.int64)\n",
        "\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_id), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        targets ={\n",
        "            'image': image,\n",
        "            'bboxes': boxes,\n",
        "            'labels': labels,\n",
        "            'keypoints': keypoints\n",
        "        }\n",
        "\n",
        "        if self.transforms is not None:\n",
        "          targets = self.transforms[self.phase](**targets)\n",
        "        \n",
        "        image = targets['image']\n",
        "        image = image / 255.0\n",
        "\n",
        "        targets = {\n",
        "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
        "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
        "            'keypoints': torch.as_tensor(\n",
        "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
        "            )\n",
        "        }\n",
        "\n",
        "        return image, targets"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k10v5KC_5IJ"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
        "    def __init__(self, data_dir, imgs, phase, transforms=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.imgs = imgs\n",
        "        self.phase = phase\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.imgs[idx]\n",
        "        # Read an image with OpenCV\n",
        "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
        "\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms[self.phase](image=img)\n",
        "            img = augmented['image']\n",
        "\n",
        "        img = img / 255.0\n",
        "        return filename, img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "  \n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXxnnANYatxE"
      },
      "source": [
        "# train dataset augmentation 을 위한 maping class\n",
        "class MapTrainDataset(Dataset): \n",
        "    \"\"\"\n",
        "    Given a dataset, creates a dataset which applies a mapping function\n",
        "    to its items (lazily, only when an item is called).\n",
        "\n",
        "    Note that data is not cloned/copied from the initial dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      targets ={\n",
        "            'image': self.dataset[index][0],\n",
        "            'bboxes': self.dataset[index][1]['boxes'],\n",
        "            'labels': self.dataset[index][1]['labels'],\n",
        "            'keypoints': self.dataset[index][1]['keypoints'][0][:,:2]\n",
        "        }\n",
        "      targets = A_transforms['train'](**targets)  \n",
        "      image =targets['image']\n",
        "      \n",
        "      #LCN\n",
        "      image = torch.tensor([np.array(img)])\n",
        "      image = LocalContrastNorm(image,radius=9)[0]\n",
        "      scaled_img = (image - torch.min(image))/(torch.max(image) - torch.min(image)) #image를 0~1로 고정\n",
        "\n",
        "      targets = {\n",
        "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
        "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
        "            'keypoints': torch.as_tensor(\n",
        "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
        "            )\n",
        "        }\n",
        "      return scaled_img, targets\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCDBvRBKauu8"
      },
      "source": [
        "# valid dataset augmentation 을 위한 maping class\n",
        "class MapValidDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, creates a dataset which applies a mapping function\n",
        "    to its items (lazily, only when an item is called).\n",
        "\n",
        "    Note that data is not cloned/copied from the initial dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      targets ={\n",
        "            'image': self.dataset[index][0],\n",
        "            'bboxes': self.dataset[index][1]['boxes'],\n",
        "            'labels': self.dataset[index][1]['labels'],\n",
        "            'keypoints': self.dataset[index][1]['keypoints'][0][:,:2]\n",
        "        }\n",
        "      targets = A_transforms['val'](**targets)  \n",
        "      image =targets['image']\n",
        "\n",
        "      #LCN\n",
        "      image = torch.tensor([np.array(img)])\n",
        "      image = LocalContrastNorm(image,radius=9)[0]\n",
        "      scaled_img = (image - torch.min(image))/(torch.max(image) - torch.min(image)) #image를 0~1로 고정\n",
        "\n",
        "      targets = {\n",
        "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
        "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
        "            'keypoints': torch.as_tensor(\n",
        "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
        "            )\n",
        "        }\n",
        "      return scaled_img, targets\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUTtEMp_azYx"
      },
      "source": [
        "np.random.seed(42)\n",
        "# 항상 동일한 valid set을 설정\n",
        "\n",
        "train_data = KeypointDataset(data_dir = os.path.join(root_dir,train_dir),label_path = os.path.join(root_dir,train_df_csv))\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(0.1 * num_train))\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "tng_data = torch.utils.data.Subset(train_data, train_idx)\n",
        "val_data = torch.utils.data.Subset(train_data, valid_idx)\n",
        "\n",
        "tng_data_tf = MapTrainDataset(tng_data)\n",
        "val_data_tf = MapValidDataset(val_data)\n",
        "# maping for augmentation\n",
        "\n",
        "train_loader = DataLoader(tng_data_tf, batch_size=batch_size,collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_data_tf, batch_size=batch_size,collate_fn=collate_fn)\n",
        "# data load"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdhSQTHaPNF"
      },
      "source": [
        "## Local Contrast Normalization\n",
        "이론링크 : https://deepestdocs.readthedocs.io/en/latest/003_image_processing/0030/  \n",
        "코드링크 : https://github.com/dibyadas/Visualize-Normalizations/blob/master/LocalContrastNorm.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB8TRdM0P785"
      },
      "source": [
        "# augmentation \n",
        "A_transforms = {\n",
        "    'train':\n",
        "        A.Compose([\n",
        "            A.Resize(224, 224, always_apply=True),\n",
        "            A.Rotate(limit=40,p=0),\n",
        "            A.OneOf([A.HorizontalFlip(p=1),\n",
        "                     A.RandomRotate90(p=1),\n",
        "                     A.VerticalFlip(p=1)            \n",
        "            ], p=0),\n",
        "            A.OneOf([A.MotionBlur(p=1),\n",
        "                     A.GaussNoise(p=1)                 \n",
        "            ], p=0),\n",
        "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ],  bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
        "            keypoint_params=A.KeypointParams(format='xy')) ,\n",
        "    \n",
        "    'val':\n",
        "        A.Compose([\n",
        "            A.Resize(224, 224, always_apply=True),\n",
        "            #A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
        "            keypoint_params=A.KeypointParams(format='xy')),\n",
        "    \n",
        "    'test':\n",
        "        A.Compose([\n",
        "            A.Resize(224, 224, always_apply=True),\n",
        "          \n",
        "          #  A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "}"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNsbNzxF9FzK"
      },
      "source": [
        "# 4. 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFEQIr04BUkV"
      },
      "source": [
        "model = get_model()\n",
        "model.cuda()\n",
        "set_parameter_requires_grad(model,feature_extracting)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
        "#patience만큼 loss가 향상되지 않으면 learning_rate에 factor을 곱해줌 \n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1, patience = 5, verbose=True)\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prTc7fDxaPNM",
        "outputId": "1cefa78f-f7f6-4248-9615-29dfb03973fb"
      },
      "source": [
        "#gpu 메모리 확인\n",
        "print(get_gpu_memory_map())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 761}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngd02azm9V3N"
      },
      "source": [
        "## 4.1 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OGzi1fg0qS7"
      },
      "source": [
        "# 4.1 model load\n",
        "\n",
        "# model = get_model()\n",
        "\n",
        "# criterion = nn.MSELoss()\n",
        "# #optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1, patience = 5, verbose=True)\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/data/best_model_0.pt')\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "# loss = checkpoint['loss']\n",
        "# model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcbyAMmJHTb5"
      },
      "source": [
        "# 5. train / save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3f50301cc73d4fe8b2df1a847e33e16b",
            "7743834c4f9548d291d4fbe0b872b48d",
            "45ce974aa7ed470da7b582e3d9355ad4",
            "24e6a115345840589050dd9b63cf373b",
            "1ce1a4eac2b74c62b06c9a38234f8e09",
            "3a61407e06d74e3ca2cf2b89bd3c2aae",
            "9e46a458c19b4f5c8aab833b4c70d51e",
            "bd810022b3f542cdacf394526333d331"
          ]
        },
        "id": "KtbPT0hgBb4x",
        "outputId": "3dd93e45-7b51-40bd-8832-3e979a80bcab"
      },
      "source": [
        "min_loss = 9999\n",
        "\n",
        "early_stopping_idx = 0;\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    cal_loss = []\n",
        "    loss_for_validation = []\n",
        "    losses = []\n",
        "    loop = tqdm(train_loader)\n",
        "    for i, (images, targets) in enumerate(loop):\n",
        "        images = list(image.float().to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        optimizer.zero_grad()\n",
        "        #loss = criterion(model(images), targets)\n",
        "        losses = model(images,targets)\n",
        "        loss = sum(loss for loss in losses.values())\n",
        "        loss.backward()\n",
        "        cal_loss.append(loss)\n",
        "        if (i+1) % 10 == 0:\n",
        "          print(f'| epoch: {epoch} | loss: {loss.item():.4f}')\n",
        "          print()\n",
        "    # validation\n",
        "    with torch.no_grad():\n",
        "      for k, (val_images, val_targets) in enumerate(val_loader):\n",
        "        val_images = list(val_image.float().to(device) for val_image in val_images)\n",
        "        val_targets = [{k: v.to(device) for k, v in t.items()} for t in val_targets]\n",
        "        val_losses = model(val_images,val_targets)\n",
        "        val_loss = sum(loss for loss in val_losses.values())\n",
        "        loss_for_validation.append(val_loss)\n",
        "\n",
        "    mean_val_loss = sum(loss_for_validation) / len(loss_for_validation)\n",
        "    mean_loss = sum(cal_loss) / len(cal_loss)\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(mean_loss)\n",
        "    writer.add_scalar(\"Loss/val\", mean_val_loss,epoch)\n",
        "    writer.add_scalar(\"Loss/train\", mean_loss, epoch)\n",
        "    writer.add_scalar(\"learning_rate\", lr, epoch)\n",
        "    print(f'| epoch: {epoch} | loss: {mean_loss:.4f} | lr : {lr} | validation loss : {mean_val_loss:.4f}')\n",
        "    print()\n",
        "        \n",
        "    if mean_val_loss < min_loss:\n",
        "      early_stopping_idx = 0\n",
        "      min_loss = mean_val_loss\n",
        "      torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': loss}\n",
        "              ,os.path.join(root_dir,'best_model.pt'))\n",
        "    else:\n",
        "      print(\"Val_loss_increase\")\n",
        "      early_stopping_idx +=1\n",
        "      if early_stopping_idx == 5:\n",
        "        print(\"early stop\")\n",
        "        break\n",
        "writer.flush()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f50301cc73d4fe8b2df1a847e33e16b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch: 0 | loss: 8.9353\n",
            "\n",
            "| epoch: 0 | loss: 9.0488\n",
            "\n",
            "| epoch: 0 | loss: 9.2625\n",
            "\n",
            "| epoch: 0 | loss: 8.8593\n",
            "\n",
            "| epoch: 0 | loss: 9.0485\n",
            "\n",
            "| epoch: 0 | loss: 8.9823\n",
            "\n",
            "| epoch: 0 | loss: 8.4676\n",
            "\n",
            "| epoch: 0 | loss: 8.6644\n",
            "\n",
            "| epoch: 0 | loss: 8.8776\n",
            "\n",
            "| epoch: 0 | loss: 9.0534\n",
            "\n",
            "| epoch: 0 | loss: 8.5640\n",
            "\n",
            "| epoch: 0 | loss: 8.8802\n",
            "\n",
            "| epoch: 0 | loss: 9.0533\n",
            "\n",
            "| epoch: 0 | loss: 8.9458\n",
            "\n",
            "| epoch: 0 | loss: 9.4870\n",
            "\n",
            "| epoch: 0 | loss: 9.2338\n",
            "\n",
            "| epoch: 0 | loss: 8.6757\n",
            "\n",
            "| epoch: 0 | loss: 8.9896\n",
            "\n",
            "| epoch: 0 | loss: 8.8734\n",
            "\n",
            "| epoch: 0 | loss: 9.1177\n",
            "\n",
            "| epoch: 0 | loss: 8.7459\n",
            "\n",
            "| epoch: 0 | loss: 8.5495\n",
            "\n",
            "| epoch: 0 | loss: 8.4516\n",
            "\n",
            "| epoch: 0 | loss: 9.3503\n",
            "\n",
            "| epoch: 0 | loss: 9.2730\n",
            "\n",
            "| epoch: 0 | loss: 8.9086\n",
            "\n",
            "| epoch: 0 | loss: 9.2234\n",
            "\n",
            "| epoch: 0 | loss: 9.2130\n",
            "\n",
            "| epoch: 0 | loss: 9.2519\n",
            "\n",
            "| epoch: 0 | loss: 9.0454\n",
            "\n",
            "| epoch: 0 | loss: 9.2513\n",
            "\n",
            "| epoch: 0 | loss: 9.0748\n",
            "\n",
            "| epoch: 0 | loss: 8.5665\n",
            "\n",
            "| epoch: 0 | loss: 9.2086\n",
            "\n",
            "| epoch: 0 | loss: 8.9064\n",
            "\n",
            "| epoch: 0 | loss: 8.6570\n",
            "\n",
            "| epoch: 0 | loss: 9.3475\n",
            "\n",
            "| epoch: 0 | loss: 8.1209\n",
            "\n",
            "| epoch: 0 | loss: 9.2112\n",
            "\n",
            "| epoch: 0 | loss: 9.3031\n",
            "\n",
            "| epoch: 0 | loss: 9.0798\n",
            "\n",
            "| epoch: 0 | loss: 8.9737\n",
            "\n",
            "| epoch: 0 | loss: 8.9718\n",
            "\n",
            "| epoch: 0 | loss: 9.2948\n",
            "\n",
            "| epoch: 0 | loss: 9.3457\n",
            "\n",
            "| epoch: 0 | loss: 8.7325\n",
            "\n",
            "| epoch: 0 | loss: 8.5280\n",
            "\n",
            "\n",
            "tensor(8.7622, device='cuda:0')\n",
            "tensor(9.2125, device='cuda:0')\n",
            "tensor(9.4005, device='cuda:0')\n",
            "tensor(8.9290, device='cuda:0')\n",
            "tensor(8.8828, device='cuda:0')\n",
            "tensor(9.0958, device='cuda:0')\n",
            "tensor(8.8060, device='cuda:0')\n",
            "tensor(8.7770, device='cuda:0')\n",
            "tensor(8.8564, device='cuda:0')\n",
            "tensor(9.1107, device='cuda:0')\n",
            "tensor(8.4774, device='cuda:0')\n",
            "tensor(8.5744, device='cuda:0')\n",
            "tensor(8.8258, device='cuda:0')\n",
            "tensor(9.0510, device='cuda:0')\n",
            "tensor(8.8186, device='cuda:0')\n",
            "tensor(9.0708, device='cuda:0')\n",
            "tensor(8.9895, device='cuda:0')\n",
            "tensor(9.2267, device='cuda:0')\n",
            "tensor(8.6050, device='cuda:0')\n",
            "tensor(8.8378, device='cuda:0')\n",
            "tensor(9.3936, device='cuda:0')\n",
            "tensor(8.5716, device='cuda:0')\n",
            "tensor(8.7951, device='cuda:0')\n",
            "tensor(9.2220, device='cuda:0')\n",
            "tensor(9.3073, device='cuda:0')\n",
            "tensor(8.9703, device='cuda:0')\n",
            "tensor(9.0314, device='cuda:0')\n",
            "tensor(9.3537, device='cuda:0')\n",
            "tensor(8.9499, device='cuda:0')\n",
            "tensor(8.7714, device='cuda:0')\n",
            "tensor(8.8242, device='cuda:0')\n",
            "tensor(9.0902, device='cuda:0')\n",
            "tensor(9.2103, device='cuda:0')\n",
            "tensor(9.1758, device='cuda:0')\n",
            "tensor(9.0213, device='cuda:0')\n",
            "tensor(9.2967, device='cuda:0')\n",
            "tensor(8.6759, device='cuda:0')\n",
            "tensor(9.4926, device='cuda:0')\n",
            "tensor(9.4049, device='cuda:0')\n",
            "tensor(8.9728, device='cuda:0')\n",
            "tensor(9.0931, device='cuda:0')\n",
            "tensor(9.0549, device='cuda:0')\n",
            "tensor(8.8724, device='cuda:0')\n",
            "tensor(9.0779, device='cuda:0')\n",
            "tensor(9.0261, device='cuda:0')\n",
            "tensor(9.4255, device='cuda:0')\n",
            "tensor(9.2591, device='cuda:0')\n",
            "tensor(9.1299, device='cuda:0')\n",
            "tensor(9.2596, device='cuda:0')\n",
            "tensor(9.1261, device='cuda:0')\n",
            "tensor(8.8951, device='cuda:0')\n",
            "tensor(8.6141, device='cuda:0')\n",
            "tensor(9.2315, device='cuda:0')\n",
            "| epoch: 0 | loss: 9.0193 | lr : 0.0001 | validation loss : 9.0171\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9bfd7b3edf91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m               \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m               'loss': loss}\n\u001b[0;32m---> 48\u001b[0;31m               ,os.path.join(root_dir,'model/best_model.pt'))\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Early Stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/model/best_model.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jthRVnkR7Cmf",
        "outputId": "bd755a97-4bf4-4060-fe71-22a321a92011"
      },
      "source": [
        "sum(cal_loss) / len(cal_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.0262, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xclMHSrPExql",
        "outputId": "3e5da034-782c-45d8-eae1-93a789c1ed62"
      },
      "source": [
        "val_losses"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss_box_reg': tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>),\n",
              " 'loss_classifier': tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " 'loss_keypoint': tensor(8.0329, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " 'loss_objectness': tensor(0.6896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
              " 'loss_rpn_box_reg': tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWwzPxw6aPNP"
      },
      "source": [
        "## tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eULC1lNaPNP"
      },
      "source": [
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHUx5v3KaPNP",
        "outputId": "a97c9508-2830-4b76-c5e8-a253656ea992"
      },
      "source": [
        "!tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\n",
            "TensorBoard 2.4.0 at http://localhost:6007/ (Press CTRL+C to quit)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCEUFVH-HyY"
      },
      "source": [
        "# 6. test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC9Nh_cM9QP5"
      },
      "source": [
        "#추론\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "files = []\n",
        "with torch.no_grad():\n",
        "  loop = tqdm(test_loader)\n",
        "  for filenames, inputs in loop:\n",
        "    pred = model(inputs.to(device))\n",
        "    # x means pred[0],pred[1],-----,pred[batch]\n",
        "    predictions = [x['keypoints'][0][:,:2].reshape(-1).detach().cpu().numpy() for x in pred]\n",
        "    files.extend(filenames)\n",
        "    for prediction in predictions:\n",
        "      all_predictions.append(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbTm7hKI-chM"
      },
      "source": [
        "# 7. 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9q5r82m19Lf"
      },
      "source": [
        "all_predictions = np.array(all_predictions)\n",
        "for i in range(all_predictions.shape[0]):\n",
        "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= 300 / 1920\n",
        "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= 150 / 1080\n",
        "df_sub = pd.read_csv(os.path.join(root_dir,'data/sample_submission.csv'))\n",
        "df = pd.DataFrame(columns=df_sub.columns)\n",
        "df['image'] = files\n",
        "df.iloc[:, 1:] = all_predictions\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhWB90MNAXeA"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "timeday = str(now)[5:10]\n",
        "df.to_csv(os.path.join(root_dir,f'data/submission_{timeday}.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87HPtH5u283R"
      },
      "source": [
        "# train 데이터 확인\n",
        "train_image , target = train_data.__getitem__(0)\n",
        "np.array(train_image).shape\n",
        "target_array = np.array(target['keypoints'][0][:,:2])\n",
        "plt.imshow(np.array(train_image).transpose(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M8aSeedlH9M"
      },
      "source": [
        "#test 확인\n",
        "filename, test_img = test_data.__getitem__(0)\n",
        "plt.imshow(np.array(test_img).transpose(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYbQroCoYa90"
      },
      "source": [
        "# def draw_keypoints(\n",
        "#     image: np.ndarray,\n",
        "#     keypoints: np.ndarray,\n",
        "#     edges: List[Tuple[int, int]] = None,\n",
        "#     keypoint_names: Dict[int, str] = None, \n",
        "#     boxes: bool = True,\n",
        "#     dpi: int = 200\n",
        "# ) -> None:\n",
        "#     \"\"\"\n",
        "#     Args:\n",
        "#         image (ndarray): [H, W, C]\n",
        "#         keypoints (ndarray): [N, 3]\n",
        "#         edges (List(Tuple(int, int))): \n",
        "#     \"\"\"\n",
        "#     np.random.seed(42)\n",
        "#     colors = {k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\n",
        "\n",
        "#     if boxes:\n",
        "#         x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
        "#         x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
        "#         cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n",
        "\n",
        "#     for i, keypoint in enumerate(keypoints):\n",
        "#         cv2.circle(\n",
        "#             image, \n",
        "#             tuple(keypoint), \n",
        "#             3,(255,0,0), thickness=3, lineType=cv2.FILLED)\n",
        "\n",
        "#         if keypoint_names is not None:\n",
        "#             cv2.putText(\n",
        "#                 image, \n",
        "#                 f'{i}: {keypoint_names[i]}', \n",
        "#                 tuple(keypoint), \n",
        "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "#     if edges is not None:\n",
        "#         for i, edge in enumerate(edges):\n",
        "#             cv2.line(\n",
        "#                 image, \n",
        "#                 tuple(keypoints[edge[0]]), \n",
        "#                 tuple(keypoints[edge[1]]),\n",
        "#                 colors.get(edge[0]), 3, lineType=cv2.LINE_AA)\n",
        "\n",
        "#     fig, ax = plt.subplots(dpi=dpi)\n",
        "#     ax.imshow(image)\n",
        "#     ax.axis('off')\n",
        "#     plt.show()\n",
        "#     keypoints = target_array\n",
        "# keypoint_names = {\n",
        "#     0: 'nose',\n",
        "#     1: 'left_eye',\n",
        "#     2: 'right_eye',\n",
        "#     3: 'left_ear', \n",
        "#     4: 'right_ear', \n",
        "#     5: 'left_shoulder', \n",
        "#     6: 'right_shoulder',\n",
        "#     7: 'left_elbow', \n",
        "#     8: 'right_elbow',\n",
        "#     9: 'left_wrist', \n",
        "#     10: 'right_wrist',\n",
        "#     11: 'left_hip', \n",
        "#     12: 'right_hip',\n",
        "#     13: 'left_knee', \n",
        "#     14: 'right_knee',\n",
        "#     15: 'left_ankle', \n",
        "#     16: 'right_ankle',\n",
        "#     17: 'neck', \n",
        "#     18: 'left_palm', \n",
        "#     19: 'right_palm', \n",
        "#     20: 'spine2(back)',\n",
        "#     21: 'spine1(waist)', \n",
        "#     22: 'left_instep',\n",
        "#     23: 'right_instep'\n",
        "# }\n",
        "\n",
        "# edges = [\n",
        "#     (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (9, 18),\n",
        "#     (10, 19), (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\n",
        "#     (14, 16), (15, 22), (16, 23), (20, 21), (5, 6), (5, 11),\n",
        "#     (6, 12), (11, 12), (17, 20), (20, 21), \n",
        "# ]\n",
        "# draw_keypoints(np.array(train_image).transpose(1,2,0), keypoints, edges, keypoint_names, boxes=False, dpi=400)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}